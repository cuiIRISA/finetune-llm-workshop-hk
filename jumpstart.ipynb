{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc11ff86-ff76-475c-a6b6-b05f81640a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.199.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.33.6)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.24.4)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.19.1)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.5.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.18)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.22.0)\n",
      "Requirement already satisfied: fastapi==0.95.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.95.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.0)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker) (1.10.13)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi==0.95.2->sagemaker) (0.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker) (0.14.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.6 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.33.6)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2023.7.22)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker) (0.58.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (3.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33a20aa9-84cc-430e-ab74-f1417bd1536d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcd5c3da-2f54-4aec-ad32-bdc1b2232c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  9543\n",
      "Number of evaluation samples:  2388\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and evaluation set\n",
    "train_dataset = dataset['train']\n",
    "# randomize the training set\n",
    "train_dataset = train_dataset.shuffle(seed=42)\n",
    "\n",
    "\n",
    "eval_dataset = dataset['validation']\n",
    "\n",
    "print('Number of training samples: ', len(train_dataset))\n",
    "print('Number of evaluation samples: ', len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91018b4f-c9e4-49e6-a24b-86079ca580a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABEL = [\"Bearish\",\"Bullish\",\"Neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33b1139e-ece6-4088-8837-40272e22e75e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_text = \"Please categorize the following Twitter financial news into one of these three categories: Bearish, Bullish, or Neutral\" \n",
    "def json_dataset(dataset):\n",
    "    record = []\n",
    "    for index in range(len(dataset)):\n",
    "        single_prompt_record = ({\"instruction\": prompt_text,\"context\": train_dataset[index][\"text\"], \"response\": LABEL[int(dataset[index][\"label\"])]})\n",
    "        # Add this list as a new element in record\n",
    "        record.append(single_prompt_record)\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd4a0470-4788-4613-bcc7-cc6cc204821f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Stocks - Tiffany, Disney, Tesla Rise Premarket; Uber Falls',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7c34851-abe0-45b8-9462-6dae09bcce25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Please categorize the following Twitter financial news into one of these three categories: Bearish, Bullish, or Neutral',\n",
       " 'context': 'Stocks - Tiffany, Disney, Tesla Rise Premarket; Uber Falls',\n",
       " 'response': 'Neutral'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_record = json_dataset(train_dataset)\n",
    "training_record[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "023f2b07-a6ce-40f7-992c-b0b0a70a2aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a name for the output file\n",
    "output_file_name = './data/jumpstart-training.jsonl'\n",
    "# Use 'with' to ensure the file gets closed after writing\n",
    "with open(output_file_name, 'w') as outfile:\n",
    "    # Use json.dump to write pdfText to the file\n",
    "    for entry in training_record:\n",
    "        json.dump(entry,outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e807123-d734-4411-b022-28a0db11b2d8",
   "metadata": {},
   "source": [
    "### Upload the dataset to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d43f0448-a7a9-40c9-bd33-adbc748b1184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Uploaded data to s3://sagemaker-us-east-1-707684582322/bedrock/jumpstart/fine-tuning\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# create a sagemaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# get the default bucket\n",
    "bucket = sagemaker_session.default_bucket() \n",
    "\n",
    "# specify the file name in S3 you want to upload\n",
    "file_name = 'bedrock/jumpstart/fine-tuning' \n",
    "\n",
    "# specify the local path of the file you want to upload\n",
    "local_path = './data/jumpstart-training.jsonl'\n",
    "\n",
    "# upload the file to S3\n",
    "sagemaker_session.upload_data(path=local_path, bucket=bucket, key_prefix=file_name)\n",
    "\n",
    "print('Uploaded data to s3://{}/{}'.format(bucket, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d16ce2-3684-47f2-b26c-30041563c784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229f335-8f8d-45fc-a9cb-335f12d48c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce9cc078-869f-49cf-893c-3f21d80e3eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"./data/template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afb482e2-693b-497d-805a-688fe6381be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgeneration-llama-2-7b\"\n",
    "model_version = \"3.*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7df8d003-924c-4014-9da9-14954229d1aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_location = \"s3://sagemaker-us-east-1-707684582322/bedrock/jumpstart/fine-tuning/jumpstart-training.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4f27e91-4942-44b0-ab40-ba8ddf235c36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:Model 'meta-textgeneration-llama-2-7b' requires accepting end-user license agreement (EULA). See https://jumpstart-cache-prod-us-east-1.s3.us-east-1.amazonaws.com/fmhMetadata/eula/llamaEula.txt for terms of use.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for training job. Defaulting to ml.g5.12xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},\n",
    "    disable_output_compression=True,  # For Llama-2-70b, add instance_type = \"ml.g5.48xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"2\", max_input_length=\"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36b132eb-2ef5-431e-9084-31364e624bed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-2-7b-2023-12-05-06-36-22-790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-05 06:36:22 Starting - Starting the training job...\n",
      "2023-12-05 06:36:51 Starting - Preparing the instances for training..........................................\n",
      "2023-12-05 06:43:35 Downloading - Downloading input data..................\n",
      "2023-12-05 06:46:56 Training - Downloading the training image..................\n",
      "2023-12-05 06:49:32 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-12-05 06:50:28,199 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-12-05 06:50:28,273 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-05 06:50:28,282 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-12-05 06:50:28,284 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-12-05 06:50:36,167 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.21.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch-triton/pytorch_triton-2.1.0+6e4932cda8-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0.dev20231104+cu118-cp310-cp310-linux_x86_64.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.31.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.1.3-py2.py3-none-any.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from pytorch-triton==2.1.0+6e4932cda8->-r requirements.txt (line 17)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 26)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0.dev20231104+cu118->-r requirements.txt (line 25)) (1.3.0)\u001b[0m\n",
      "\u001b[34mtokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=77a1a12bc11c6b74f2a3ddd395baa21067d972422a22638b22b19336817202b2\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, safetensors, Brotli, bitsandbytes, tokenize-rt, termcolor, scipy, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, pyzstd, pytorch-triton, pyppmd, pycryptodomex, pybcj, pathspec, mypy-extensions, multivolumefile, loralib, inflate64, torch, py7zr, fire, black, transformers, accelerate, peft, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.10.1\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.10.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.10.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0.dev20231104+cu118 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.21.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 fire-0.5.0 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pytorch-triton-2.1.0+6e4932cda8 pyzstd-0.15.9 safetensors-0.3.1 sagemaker-jumpstart-huggingface-script-utilities-1.1.3 sagemaker-jumpstart-script-utilities-1.1.9 scipy-1.11.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 torch-2.2.0.dev20231104+cu118 transformers-4.31.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-12-05 06:51:30,927 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-12-05 06:51:30,927 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-12-05 06:51:31,034 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-05 06:51:31,118 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-05 06:51:31,200 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-12-05 06:51:31,209 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"2\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-2-7b-2023-12-05-06-36-22-790\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"2\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"enable_fsdp\":\"True\",\"epoch\":\"2\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-2-7b-2023-12-05-06-36-22-790\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--enable_fsdp\",\"True\",\"--epoch\",\"2\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=2\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --enable_fsdp True --epoch 2 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2023-12-05 06:51:31,236 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Identify file serving.properties in the un-tar directory /opt/ml/additonals3data. Copying it over to /opt/ml/model for model deployment after training is finished.\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '2', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2023-12-05 06:51:36,539] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2023-12-05 06:51:36,539] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2023-12-05 06:51:36,539] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2023-12-05 06:51:36,539] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|| 1/1 [00:00<00:00, 14873.42it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|| 1/1 [00:00<00:00, 2097.15it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9543 examples [00:00, 412548.11 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|        | 2024/9543 [00:00<00:00, 20126.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|       | 2099/9543 [00:00<00:00, 20876.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|       | 2080/9543 [00:00<00:00, 20660.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|       | 2092/9543 [00:00<00:00, 20796.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|     | 4085/9543 [00:00<00:00, 20405.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|     | 4251/9543 [00:00<00:00, 21249.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|     | 4228/9543 [00:00<00:00, 21139.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|     | 4258/9543 [00:00<00:00, 21298.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|   | 6411/9543 [00:00<00:00, 21402.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|   | 6377/9543 [00:00<00:00, 21296.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|   | 6422/9543 [00:00<00:00, 21449.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|  | 7133/9543 [00:00<00:00, 20350.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%| | 8581/9543 [00:00<00:00, 21515.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%| | 8584/9543 [00:00<00:00, 21510.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 21352.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 20272.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 20248.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 21166.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 21363.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 21088.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|         | 1000/9543 [00:00<00:00, 9969.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|        | 2000/9543 [00:00<00:00, 14965.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|        | 2000/9543 [00:00<00:00, 12375.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|        | 2000/9543 [00:00<00:00, 11854.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|      | 3000/9543 [00:00<00:00, 14391.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|     | 4000/9543 [00:00<00:00, 16424.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|     | 4000/9543 [00:00<00:00, 13862.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|    | 5000/9543 [00:00<00:00, 16272.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|     | 4000/9543 [00:00<00:00, 11486.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|   | 6000/9543 [00:00<00:00, 15583.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|   | 6000/9543 [00:00<00:00, 15117.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|  | 7000/9543 [00:00<00:00, 16390.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|   | 6000/9543 [00:00<00:00, 13660.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%| | 8000/9543 [00:00<00:00, 15914.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%| | 8000/9543 [00:00<00:00, 16421.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|| 9000/9543 [00:00<00:00, 16960.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%| | 8000/9543 [00:00<00:00, 15008.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 16158.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 15451.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 15687.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:00<00:00, 14668.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9543 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|         | 1000/9543 [00:00<00:01, 8525.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|         | 1000/9543 [00:00<00:01, 8059.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|         | 1000/9543 [00:00<00:01, 8055.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|         | 1000/9543 [00:00<00:01, 8320.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|        | 2000/9543 [00:00<00:00, 8717.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|        | 2000/9543 [00:00<00:00, 8184.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|        | 2000/9543 [00:00<00:00, 8163.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|        | 2000/9543 [00:00<00:00, 8584.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|      | 3000/9543 [00:00<00:00, 8742.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|      | 3000/9543 [00:00<00:00, 8243.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|      | 3000/9543 [00:00<00:00, 8193.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|      | 3000/9543 [00:00<00:00, 8655.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|     | 4000/9543 [00:00<00:00, 8751.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|     | 4000/9543 [00:00<00:00, 8255.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|     | 4000/9543 [00:00<00:00, 8197.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|     | 4000/9543 [00:00<00:00, 8695.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|    | 5000/9543 [00:00<00:00, 8778.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|    | 5000/9543 [00:00<00:00, 8280.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|    | 5000/9543 [00:00<00:00, 8240.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|    | 5000/9543 [00:00<00:00, 8720.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|   | 6000/9543 [00:00<00:00, 8811.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|   | 6000/9543 [00:00<00:00, 8303.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|   | 6000/9543 [00:00<00:00, 8257.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|   | 6000/9543 [00:00<00:00, 8722.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|  | 7000/9543 [00:00<00:00, 8794.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%| | 8000/9543 [00:00<00:00, 8795.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|  | 7000/9543 [00:00<00:00, 6451.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|  | 7000/9543 [00:00<00:00, 6688.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|  | 7000/9543 [00:00<00:00, 6424.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|| 9000/9543 [00:01<00:00, 8765.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:01<00:00, 8743.48 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:  84%| | 8000/9543 [00:01<00:00, 6977.98 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  84%| | 8000/9543 [00:01<00:00, 7224.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%| | 8000/9543 [00:01<00:00, 6914.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|| 9000/9543 [00:01<00:00, 7675.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|| 9000/9543 [00:01<00:00, 7370.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|| 9000/9543 [00:01<00:00, 7267.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:01<00:00, 7925.43 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:01<00:00, 7589.96 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|| 9543/9543 [00:01<00:00, 7530.42 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Split the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:09<00:09,  9.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:09<00:09,  9.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:09<00:09,  9.76s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:09<00:09,  9.75s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.28s/it]#015Loading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.95s/it]#015Loading checkpoint shards: 100%|| 2/2 [00:11<00:00,  5.96s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 6738.415616 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mtrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/52 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/52 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 836\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 209\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/52 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/52 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda11.8\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m         #033[0m| 1/52 [00:10<09:08, 10.75s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m         #033[0m| 1/52 [00:10<09:06, 10.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m         #033[0m| 1/52 [00:10<08:55, 10.49s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.3976290225982666\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m         #033[0m| 1/52 [00:10<08:55, 10.50s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.3457294702529907\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m         #033[0m| 2/52 [00:20<08:31, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m         #033[0m| 2/52 [00:20<08:26, 10.13s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m         #033[0m| 2/52 [00:20<08:26, 10.13s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m         #033[0m| 2/52 [00:20<08:31, 10.24s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m         #033[0m| 3/52 [00:30<08:10, 10.00s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m         #033[0m| 3/52 [00:30<08:12, 10.06s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m         #033[0m| 3/52 [00:30<08:12, 10.05s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.3266792297363281\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m         #033[0m| 3/52 [00:30<08:10, 10.00s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.1246764659881592\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m         #033[0m| 4/52 [00:40<07:57,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m         #033[0m| 4/52 [00:40<07:57,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m         #033[0m| 4/52 [00:40<07:58,  9.98s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m         #033[0m| 4/52 [00:40<07:58,  9.97s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 1.2651804685592651\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m         #033[0m| 5/52 [00:49<07:45,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m         #033[0m| 5/52 [00:49<07:45,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m         #033[0m| 5/52 [00:50<07:46,  9.94s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m         #033[0m| 5/52 [00:50<07:46,  9.93s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.1307857036590576\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m        #033[0m| 6/52 [00:59<07:34,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m        #033[0m| 6/52 [01:00<07:35,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m        #033[0m| 6/52 [00:59<07:35,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m        #033[0m| 6/52 [01:00<07:35,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m        #033[0m| 7/52 [01:09<07:24,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m        #033[0m| 7/52 [01:09<07:24,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.2062875032424927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m        #033[0m| 7/52 [01:09<07:24,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m        #033[0m| 7/52 [01:09<07:24,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m        #033[0m| 8/52 [01:19<07:14,  9.87s/it]#015Training Epoch0:  15%|#033[34m        #033[0m| 8/52 [01:19<07:14,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m        #033[0m| 8/52 [01:19<07:13,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.0786159038543701\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m        #033[0m| 8/52 [01:19<07:14,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m        #033[0m| 9/52 [01:29<07:03,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m        #033[0m| 9/52 [01:29<07:04,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 1.1069399118423462\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m        #033[0m| 9/52 [01:29<07:04,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m        #033[0m| 9/52 [01:29<07:03,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m        #033[0m| 10/52 [01:39<06:53,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m        #033[0m| 10/52 [01:39<06:54,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m        #033[0m| 10/52 [01:39<06:54,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 1.0404208898544312\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m        #033[0m| 10/52 [01:39<06:53,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 1.0297458171844482\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m        #033[0m| 11/52 [01:48<06:43,  9.85s/it]#015Training Epoch0:  21%|#033[34m        #033[0m| 11/52 [01:48<06:43,  9.85s/it]#015Training Epoch0:  21%|#033[34m        #033[0m| 11/52 [01:49<06:43,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m        #033[0m| 11/52 [01:49<06:43,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m       #033[0m| 12/52 [01:58<06:33,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m       #033[0m| 12/52 [01:59<06:33,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m       #033[0m| 12/52 [01:59<06:33,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 0.997044563293457\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m       #033[0m| 12/52 [01:58<06:33,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 1.0108263492584229\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m       #033[0m| 13/52 [02:09<06:25,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m       #033[0m| 13/52 [02:08<06:25,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m       #033[0m| 13/52 [02:08<06:25,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m       #033[0m| 13/52 [02:09<06:25,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m       #033[0m| 14/52 [02:19<06:16,  9.91s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.9128504395484924\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m       #033[0m| 14/52 [02:18<06:16,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m       #033[0m| 14/52 [02:18<06:16,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m       #033[0m| 14/52 [02:19<06:16,  9.91s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m       #033[0m| 15/52 [02:28<06:06,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m       #033[0m| 15/52 [02:28<06:06,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 0.9253528714179993\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m       #033[0m| 15/52 [02:28<06:06,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m       #033[0m| 15/52 [02:28<06:06,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m       #033[0m| 16/52 [02:38<05:55,  9.88s/it]#015Training Epoch0:  31%|#033[34m       #033[0m| 16/52 [02:38<05:55,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 0.8772103786468506\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m       #033[0m| 16/52 [02:38<05:55,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m       #033[0m| 16/52 [02:38<05:55,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m      #033[0m| 17/52 [02:48<05:45,  9.87s/it]#015Training Epoch0:  33%|#033[34m      #033[0m| 17/52 [02:48<05:45,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.0329363346099854\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m      #033[0m| 17/52 [02:48<05:45,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m      #033[0m| 17/52 [02:48<05:45,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m      #033[0m| 18/52 [02:58<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m      #033[0m| 18/52 [02:58<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 1.0107342004776\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m      #033[0m| 18/52 [02:58<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m      #033[0m| 18/52 [02:58<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m      #033[0m| 19/52 [03:08<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 0.9988324046134949\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m      #033[0m| 19/52 [03:08<05:25,  9.86s/it]#015Training Epoch0:  37%|#033[34m      #033[0m| 19/52 [03:08<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m      #033[0m| 19/52 [03:08<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.9021199941635132\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m      #033[0m| 20/52 [03:17<05:15,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m      #033[0m| 20/52 [03:17<05:15,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m      #033[0m| 20/52 [03:18<05:15,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m      #033[0m| 20/52 [03:18<05:15,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m      #033[0m| 21/52 [03:27<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m      #033[0m| 21/52 [03:27<05:05,  9.85s/it]#015Training Epoch0:  40%|#033[34m      #033[0m| 21/52 [03:27<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.9099134802818298\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m      #033[0m| 21/52 [03:27<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m     #033[0m| 22/52 [03:37<04:55,  9.85s/it]#015Training Epoch0:  42%|#033[34m     #033[0m| 22/52 [03:37<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 0.8532958626747131\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m     #033[0m| 22/52 [03:37<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m     #033[0m| 22/52 [03:37<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m     #033[0m| 23/52 [03:47<04:45,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m     #033[0m| 23/52 [03:47<04:45,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m     #033[0m| 23/52 [03:47<04:45,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.9377928972244263\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m     #033[0m| 23/52 [03:47<04:45,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m     #033[0m| 24/52 [03:57<04:35,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m     #033[0m| 24/52 [03:57<04:35,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m     #033[0m| 24/52 [03:57<04:35,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 0.7923669815063477\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m     #033[0m| 24/52 [03:57<04:35,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m     #033[0m| 25/52 [04:07<04:25,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m     #033[0m| 25/52 [04:07<04:25,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m     #033[0m| 25/52 [04:07<04:25,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.7386236786842346\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m     #033[0m| 25/52 [04:07<04:25,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m     #033[0m| 26/52 [04:17<04:17,  9.90s/it]#015Training Epoch0:  50%|#033[34m     #033[0m| 26/52 [04:17<04:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 0.8424022793769836\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m     #033[0m| 26/52 [04:17<04:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  50%|#033[34m     #033[0m| 26/52 [04:17<04:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m    #033[0m| 27/52 [04:27<04:08,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m    #033[0m| 27/52 [04:27<04:08,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m    #033[0m| 27/52 [04:27<04:08,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 0.836492121219635\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m    #033[0m| 27/52 [04:27<04:08,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 0.7908170223236084\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m    #033[0m| 28/52 [04:36<03:57,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m    #033[0m| 28/52 [04:37<03:57,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m    #033[0m| 28/52 [04:36<03:57,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m    #033[0m| 28/52 [04:37<03:57,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m    #033[0m| 29/52 [04:47<03:47,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m    #033[0m| 29/52 [04:46<03:47,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m    #033[0m| 29/52 [04:47<03:47,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 0.8290829062461853\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m    #033[0m| 29/52 [04:46<03:47,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m    #033[0m| 30/52 [04:56<03:37,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 0.7814582586288452\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m    #033[0m| 30/52 [04:56<03:37,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m    #033[0m| 30/52 [04:56<03:37,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m    #033[0m| 30/52 [04:56<03:37,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m    #033[0m| 31/52 [05:06<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m    #033[0m| 31/52 [05:06<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.8529503345489502\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m    #033[0m| 31/52 [05:06<03:27,  9.87s/it]#015Training Epoch0:  60%|#033[34m    #033[0m| 31/52 [05:06<03:27,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m   #033[0m| 32/52 [05:16<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m   #033[0m| 32/52 [05:16<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 0.7225728631019592\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m   #033[0m| 32/52 [05:16<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m   #033[0m| 32/52 [05:16<03:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m   #033[0m| 33/52 [05:26<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.7505502700805664\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m   #033[0m| 33/52 [05:26<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m   #033[0m| 33/52 [05:26<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m   #033[0m| 33/52 [05:26<03:07,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 0.7483586668968201\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m   #033[0m| 34/52 [05:36<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m   #033[0m| 34/52 [05:36<02:57,  9.86s/it]#015Training Epoch0:  65%|#033[34m   #033[0m| 34/52 [05:36<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m   #033[0m| 34/52 [05:36<02:57,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.6598577499389648\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m   #033[0m| 35/52 [05:45<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m   #033[0m| 35/52 [05:46<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m   #033[0m| 35/52 [05:46<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m   #033[0m| 35/52 [05:45<02:47,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m   #033[0m| 36/52 [05:55<02:37,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 0.7830498218536377\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m   #033[0m| 36/52 [05:55<02:37,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m   #033[0m| 36/52 [05:56<02:37,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m   #033[0m| 36/52 [05:56<02:37,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m   #033[0m| 37/52 [06:05<02:27,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 0.726022481918335\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m   #033[0m| 37/52 [06:05<02:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m   #033[0m| 37/52 [06:05<02:27,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m   #033[0m| 37/52 [06:05<02:27,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.6860775351524353\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m  #033[0m| 38/52 [06:15<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m  #033[0m| 38/52 [06:15<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m  #033[0m| 38/52 [06:15<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m  #033[0m| 38/52 [06:15<02:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 0.6646846532821655\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m  #033[0m| 39/52 [06:25<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m  #033[0m| 39/52 [06:25<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m  #033[0m| 39/52 [06:25<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m  #033[0m| 39/52 [06:25<02:08,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 0.6618056893348694\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m  #033[0m| 40/52 [06:35<01:58,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m  #033[0m| 40/52 [06:35<01:58,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m  #033[0m| 40/52 [06:35<01:58,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m  #033[0m| 40/52 [06:35<01:58,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m  #033[0m| 41/52 [06:45<01:49,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 0.7593618035316467\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m  #033[0m| 41/52 [06:45<01:49,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m  #033[0m| 41/52 [06:45<01:49,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m  #033[0m| 41/52 [06:45<01:49,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 0.7460530400276184\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m  #033[0m| 42/52 [06:55<01:38,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m  #033[0m| 42/52 [06:55<01:38,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m  #033[0m| 42/52 [06:55<01:38,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m  #033[0m| 42/52 [06:55<01:38,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.7186643481254578\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m #033[0m| 43/52 [07:05<01:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m #033[0m| 43/52 [07:05<01:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m #033[0m| 43/52 [07:05<01:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  83%|#033[34m #033[0m| 43/52 [07:05<01:28,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 0.7421532273292542\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m #033[0m| 44/52 [07:14<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m #033[0m| 44/52 [07:15<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m #033[0m| 44/52 [07:14<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m #033[0m| 44/52 [07:15<01:18,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m #033[0m| 45/52 [07:24<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 0.7184273600578308\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m #033[0m| 45/52 [07:24<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m #033[0m| 45/52 [07:24<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m #033[0m| 45/52 [07:24<01:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m #033[0m| 46/52 [07:34<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 0.6942635178565979\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m #033[0m| 46/52 [07:34<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m #033[0m| 46/52 [07:34<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m #033[0m| 46/52 [07:34<00:59,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m #033[0m| 47/52 [07:44<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m #033[0m| 47/52 [07:44<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m #033[0m| 47/52 [07:44<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 0.8198925256729126\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m #033[0m| 47/52 [07:44<00:49,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m#033[0m| 48/52 [07:54<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m#033[0m| 48/52 [07:54<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m#033[0m| 48/52 [07:54<00:39,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 0.7561158537864685\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m#033[0m| 48/52 [07:54<00:39,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m#033[0m| 49/52 [08:04<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 0.7702885270118713\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m#033[0m| 49/52 [08:04<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m#033[0m| 49/52 [08:04<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  94%|#033[34m#033[0m| 49/52 [08:04<00:29,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m#033[0m| 50/52 [08:14<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m#033[0m| 50/52 [08:14<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m#033[0m| 50/52 [08:13<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 0.731572151184082\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m#033[0m| 50/52 [08:13<00:19,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m#033[0m| 51/52 [08:24<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m#033[0m| 51/52 [08:23<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m#033[0m| 51/52 [08:24<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 0.73441481590271\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m#033[0m| 51/52 [08:23<00:09,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m#033[0m| 52/52 [08:34<00:00,  9.89s/it]#015Training Epoch0: 100%|#033[34m#033[0m| 52/52 [08:34<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m#033[0m| 52/52 [08:34<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m#033[0m| 52/52 [08:34<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 0.729648232460022\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m#033[0m| 52/52 [08:33<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m#033[0m| 52/52 [08:33<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m#033[0m| 52/52 [08:33<00:00,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m#033[0m| 52/52 [08:33<00:00,  9.88s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 1 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/53 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/53 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/53 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/53 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m         #033[0m| 1/53 [00:03<03:06,  3.59s/it]#015evaluating Epoch:   2%|#033[32m         #033[0m| 1/53 [00:03<03:06,  3.59s/it]#015evaluating Epoch:   2%|#033[32m         #033[0m| 1/53 [00:03<03:06,  3.58s/it]#015evaluating Epoch:   2%|#033[32m         #033[0m| 1/53 [00:03<03:06,  3.58s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m         #033[0m| 2/53 [00:07<02:58,  3.51s/it]#015evaluating Epoch:   4%|#033[32m         #033[0m| 2/53 [00:07<02:58,  3.51s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m         #033[0m| 2/53 [00:07<02:58,  3.50s/it]#015evaluating Epoch:   4%|#033[32m         #033[0m| 2/53 [00:07<02:58,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m         #033[0m| 3/53 [00:10<02:53,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m         #033[0m| 3/53 [00:10<02:53,  3.48s/it]#015evaluating Epoch:   6%|#033[32m         #033[0m| 3/53 [00:10<02:53,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m         #033[0m| 3/53 [00:10<02:53,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m         #033[0m| 4/53 [00:13<02:49,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m         #033[0m| 4/53 [00:13<02:49,  3.47s/it]#015evaluating Epoch:   8%|#033[32m         #033[0m| 4/53 [00:13<02:49,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m         #033[0m| 4/53 [00:13<02:49,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m         #033[0m| 5/53 [00:17<02:46,  3.46s/it]#015evaluating Epoch:   9%|#033[32m         #033[0m| 5/53 [00:17<02:46,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m         #033[0m| 5/53 [00:17<02:46,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m         #033[0m| 5/53 [00:17<02:46,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m        #033[0m| 6/53 [00:20<02:42,  3.46s/it]#015evaluating Epoch:  11%|#033[32m        #033[0m| 6/53 [00:20<02:42,  3.46s/it]#015evaluating Epoch:  11%|#033[32m        #033[0m| 6/53 [00:20<02:42,  3.46s/it]#015evaluating Epoch:  11%|#033[32m        #033[0m| 6/53 [00:20<02:42,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m        #033[0m| 7/53 [00:24<02:38,  3.46s/it]#015evaluating Epoch:  13%|#033[32m        #033[0m| 7/53 [00:24<02:38,  3.45s/it]#015evaluating Epoch:  13%|#033[32m        #033[0m| 7/53 [00:24<02:38,  3.46s/it]#015evaluating Epoch:  13%|#033[32m        #033[0m| 7/53 [00:24<02:38,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m        #033[0m| 8/53 [00:27<02:35,  3.45s/it]#015evaluating Epoch:  15%|#033[32m        #033[0m| 8/53 [00:27<02:35,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m        #033[0m| 8/53 [00:27<02:35,  3.45s/it]#015evaluating Epoch:  15%|#033[32m        #033[0m| 8/53 [00:27<02:35,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m        #033[0m| 9/53 [00:31<02:31,  3.45s/it]#015evaluating Epoch:  17%|#033[32m        #033[0m| 9/53 [00:31<02:31,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m        #033[0m| 9/53 [00:31<02:31,  3.45s/it]#015evaluating Epoch:  17%|#033[32m        #033[0m| 9/53 [00:31<02:31,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m        #033[0m| 10/53 [00:34<02:28,  3.45s/it]#015evaluating Epoch:  19%|#033[32m        #033[0m| 10/53 [00:34<02:28,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m        #033[0m| 10/53 [00:34<02:28,  3.45s/it]#015evaluating Epoch:  19%|#033[32m        #033[0m| 10/53 [00:34<02:28,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m        #033[0m| 11/53 [00:38<02:25,  3.46s/it]#015evaluating Epoch:  21%|#033[32m        #033[0m| 11/53 [00:38<02:25,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m        #033[0m| 11/53 [00:38<02:25,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m        #033[0m| 11/53 [00:38<02:25,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m       #033[0m| 12/53 [00:41<02:21,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m       #033[0m| 12/53 [00:41<02:21,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m       #033[0m| 12/53 [00:41<02:21,  3.46s/it]#015evaluating Epoch:  23%|#033[32m       #033[0m| 12/53 [00:41<02:21,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m       #033[0m| 13/53 [00:45<02:18,  3.45s/it]#015evaluating Epoch:  25%|#033[32m       #033[0m| 13/53 [00:45<02:18,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m       #033[0m| 13/53 [00:45<02:18,  3.45s/it]#015evaluating Epoch:  25%|#033[32m       #033[0m| 13/53 [00:45<02:18,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m       #033[0m| 14/53 [00:48<02:14,  3.45s/it]#015evaluating Epoch:  26%|#033[32m       #033[0m| 14/53 [00:48<02:14,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m       #033[0m| 14/53 [00:48<02:14,  3.45s/it]#015evaluating Epoch:  26%|#033[32m       #033[0m| 14/53 [00:48<02:14,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m       #033[0m| 15/53 [00:51<02:11,  3.46s/it]#015evaluating Epoch:  28%|#033[32m       #033[0m| 15/53 [00:51<02:11,  3.46s/it]#015evaluating Epoch:  28%|#033[32m       #033[0m| 15/53 [00:51<02:11,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m       #033[0m| 15/53 [00:51<02:11,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m       #033[0m| 16/53 [00:55<02:07,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m       #033[0m| 16/53 [00:55<02:07,  3.46s/it]#015evaluating Epoch:  30%|#033[32m       #033[0m| 16/53 [00:55<02:07,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m       #033[0m| 16/53 [00:55<02:07,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m      #033[0m| 17/53 [00:58<02:04,  3.46s/it]#015evaluating Epoch:  32%|#033[32m      #033[0m| 17/53 [00:58<02:04,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m      #033[0m| 17/53 [00:58<02:04,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m      #033[0m| 17/53 [00:58<02:04,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m      #033[0m| 18/53 [01:02<02:00,  3.46s/it]#015evaluating Epoch:  34%|#033[32m      #033[0m| 18/53 [01:02<02:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m      #033[0m| 18/53 [01:02<02:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m      #033[0m| 18/53 [01:02<02:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m      #033[0m| 19/53 [01:05<01:57,  3.45s/it]#015evaluating Epoch:  36%|#033[32m      #033[0m| 19/53 [01:05<01:57,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m      #033[0m| 19/53 [01:05<01:57,  3.45s/it]#015evaluating Epoch:  36%|#033[32m      #033[0m| 19/53 [01:05<01:57,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m      #033[0m| 20/53 [01:09<01:53,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m      #033[0m| 20/53 [01:09<01:53,  3.45s/it]#015evaluating Epoch:  38%|#033[32m      #033[0m| 20/53 [01:09<01:53,  3.45s/it]#015evaluating Epoch:  38%|#033[32m      #033[0m| 20/53 [01:09<01:53,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m      #033[0m| 21/53 [01:12<01:50,  3.45s/it]#015evaluating Epoch:  40%|#033[32m      #033[0m| 21/53 [01:12<01:50,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m      #033[0m| 21/53 [01:12<01:50,  3.45s/it]#015evaluating Epoch:  40%|#033[32m      #033[0m| 21/53 [01:12<01:50,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m     #033[0m| 22/53 [01:16<01:46,  3.45s/it]#015evaluating Epoch:  42%|#033[32m     #033[0m| 22/53 [01:16<01:46,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m     #033[0m| 22/53 [01:16<01:46,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m     #033[0m| 22/53 [01:16<01:46,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m     #033[0m| 23/53 [01:19<01:43,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m     #033[0m| 23/53 [01:19<01:43,  3.45s/it]#015evaluating Epoch:  43%|#033[32m     #033[0m| 23/53 [01:19<01:43,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m     #033[0m| 23/53 [01:19<01:43,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m     #033[0m| 24/53 [01:22<01:40,  3.45s/it]#015evaluating Epoch:  45%|#033[32m     #033[0m| 24/53 [01:22<01:40,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m     #033[0m| 24/53 [01:22<01:40,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m     #033[0m| 24/53 [01:22<01:40,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m     #033[0m| 25/53 [01:26<01:36,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m     #033[0m| 25/53 [01:26<01:36,  3.45s/it]#015evaluating Epoch:  47%|#033[32m     #033[0m| 25/53 [01:26<01:36,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m     #033[0m| 25/53 [01:26<01:36,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m     #033[0m| 26/53 [01:29<01:33,  3.46s/it]#015evaluating Epoch:  49%|#033[32m     #033[0m| 26/53 [01:29<01:33,  3.46s/it]#015evaluating Epoch:  49%|#033[32m     #033[0m| 26/53 [01:29<01:33,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m     #033[0m| 26/53 [01:29<01:33,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m     #033[0m| 27/53 [01:33<01:29,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m     #033[0m| 27/53 [01:33<01:29,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m     #033[0m| 27/53 [01:33<01:29,  3.45s/it]#015evaluating Epoch:  51%|#033[32m     #033[0m| 27/53 [01:33<01:29,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m    #033[0m| 28/53 [01:36<01:26,  3.46s/it]#015evaluating Epoch:  53%|#033[32m    #033[0m| 28/53 [01:36<01:26,  3.46s/it]#015evaluating Epoch:  53%|#033[32m    #033[0m| 28/53 [01:36<01:26,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m    #033[0m| 28/53 [01:36<01:26,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m    #033[0m| 29/53 [01:40<01:22,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m    #033[0m| 29/53 [01:40<01:22,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m    #033[0m| 29/53 [01:40<01:22,  3.46s/it]#015evaluating Epoch:  55%|#033[32m    #033[0m| 29/53 [01:40<01:22,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m    #033[0m| 30/53 [01:43<01:19,  3.45s/it]#015evaluating Epoch:  57%|#033[32m    #033[0m| 30/53 [01:43<01:19,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m    #033[0m| 30/53 [01:43<01:19,  3.45s/it]#015evaluating Epoch:  57%|#033[32m    #033[0m| 30/53 [01:43<01:19,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m    #033[0m| 31/53 [01:47<01:15,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m    #033[0m| 31/53 [01:47<01:15,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m    #033[0m| 31/53 [01:47<01:15,  3.45s/it]#015evaluating Epoch:  58%|#033[32m    #033[0m| 31/53 [01:47<01:15,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m    #033[0m| 32/53 [01:50<01:12,  3.45s/it]#015evaluating Epoch:  60%|#033[32m    #033[0m| 32/53 [01:50<01:12,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m    #033[0m| 32/53 [01:50<01:12,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m    #033[0m| 32/53 [01:50<01:12,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m   #033[0m| 33/53 [01:54<01:09,  3.45s/it]#015evaluating Epoch:  62%|#033[32m   #033[0m| 33/53 [01:54<01:09,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m   #033[0m| 33/53 [01:54<01:09,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m   #033[0m| 33/53 [01:54<01:09,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m   #033[0m| 34/53 [01:57<01:05,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m   #033[0m| 34/53 [01:57<01:05,  3.45s/it]#015evaluating Epoch:  64%|#033[32m   #033[0m| 34/53 [01:57<01:05,  3.45s/it]#015evaluating Epoch:  64%|#033[32m   #033[0m| 34/53 [01:57<01:05,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m   #033[0m| 35/53 [02:00<01:02,  3.45s/it]#015evaluating Epoch:  66%|#033[32m   #033[0m| 35/53 [02:00<01:02,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m   #033[0m| 35/53 [02:00<01:02,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m   #033[0m| 35/53 [02:00<01:02,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m   #033[0m| 36/53 [02:04<00:58,  3.45s/it]#015evaluating Epoch:  68%|#033[32m   #033[0m| 36/53 [02:04<00:58,  3.45s/it]#015evaluating Epoch:  68%|#033[32m   #033[0m| 36/53 [02:04<00:58,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m   #033[0m| 36/53 [02:04<00:58,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m   #033[0m| 37/53 [02:07<00:55,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m   #033[0m| 37/53 [02:07<00:55,  3.45s/it]#015evaluating Epoch:  70%|#033[32m   #033[0m| 37/53 [02:07<00:55,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m   #033[0m| 37/53 [02:07<00:55,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m  #033[0m| 38/53 [02:11<00:51,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m  #033[0m| 38/53 [02:11<00:51,  3.45s/it]#015evaluating Epoch:  72%|#033[32m  #033[0m| 38/53 [02:11<00:51,  3.45s/it]#015evaluating Epoch:  72%|#033[32m  #033[0m| 38/53 [02:11<00:51,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m  #033[0m| 39/53 [02:14<00:48,  3.45s/it]#015evaluating Epoch:  74%|#033[32m  #033[0m| 39/53 [02:14<00:48,  3.45s/it]#015evaluating Epoch:  74%|#033[32m  #033[0m| 39/53 [02:14<00:48,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m  #033[0m| 39/53 [02:14<00:48,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m  #033[0m| 40/53 [02:18<00:44,  3.45s/it]#015evaluating Epoch:  75%|#033[32m  #033[0m| 40/53 [02:18<00:44,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m  #033[0m| 40/53 [02:18<00:44,  3.45s/it]#015evaluating Epoch:  75%|#033[32m  #033[0m| 40/53 [02:18<00:44,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m  #033[0m| 41/53 [02:21<00:41,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m  #033[0m| 41/53 [02:21<00:41,  3.45s/it]#015evaluating Epoch:  77%|#033[32m  #033[0m| 41/53 [02:21<00:41,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m  #033[0m| 41/53 [02:21<00:41,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m  #033[0m| 42/53 [02:25<00:37,  3.45s/it]#015evaluating Epoch:  79%|#033[32m  #033[0m| 42/53 [02:25<00:37,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m  #033[0m| 42/53 [02:25<00:37,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m  #033[0m| 42/53 [02:25<00:37,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m  #033[0m| 43/53 [02:28<00:34,  3.45s/it]#015evaluating Epoch:  81%|#033[32m  #033[0m| 43/53 [02:28<00:34,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m  #033[0m| 43/53 [02:28<00:34,  3.45s/it]#015evaluating Epoch:  81%|#033[32m  #033[0m| 43/53 [02:28<00:34,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m #033[0m| 44/53 [02:32<00:31,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m #033[0m| 44/53 [02:32<00:31,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m #033[0m| 44/53 [02:32<00:31,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m #033[0m| 44/53 [02:32<00:31,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m #033[0m| 45/53 [02:35<00:27,  3.46s/it]#015evaluating Epoch:  85%|#033[32m #033[0m| 45/53 [02:35<00:27,  3.46s/it]#015evaluating Epoch:  85%|#033[32m #033[0m| 45/53 [02:35<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m #033[0m| 45/53 [02:35<00:27,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m #033[0m| 46/53 [02:38<00:24,  3.46s/it]#015evaluating Epoch:  87%|#033[32m #033[0m| 46/53 [02:38<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m #033[0m| 46/53 [02:38<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m #033[0m| 46/53 [02:38<00:24,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m #033[0m| 47/53 [02:42<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m #033[0m| 47/53 [02:42<00:20,  3.46s/it]#015evaluating Epoch:  89%|#033[32m #033[0m| 47/53 [02:42<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m #033[0m| 47/53 [02:42<00:20,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m #033[0m| 48/53 [02:45<00:17,  3.45s/it]#015evaluating Epoch:  91%|#033[32m #033[0m| 48/53 [02:45<00:17,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m #033[0m| 48/53 [02:45<00:17,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m #033[0m| 48/53 [02:45<00:17,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m#033[0m| 49/53 [02:49<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m#033[0m| 49/53 [02:49<00:13,  3.46s/it]#015evaluating Epoch:  92%|#033[32m#033[0m| 49/53 [02:49<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m#033[0m| 49/53 [02:49<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m#033[0m| 50/53 [02:52<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m#033[0m| 50/53 [02:52<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m#033[0m| 50/53 [02:52<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m#033[0m| 50/53 [02:52<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m#033[0m| 51/53 [02:56<00:06,  3.46s/it]#015evaluating Epoch:  96%|#033[32m#033[0m| 51/53 [02:56<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m#033[0m| 51/53 [02:56<00:06,  3.46s/it]#015evaluating Epoch:  96%|#033[32m#033[0m| 51/53 [02:56<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m#033[0m| 52/53 [02:59<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m#033[0m| 52/53 [02:59<00:03,  3.46s/it]#015evaluating Epoch:  98%|#033[32m#033[0m| 52/53 [02:59<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m#033[0m| 52/53 [02:59<00:03,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.46s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(2.0242, device='cuda:0') eval_epoch_loss=tensor(0.7052, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 0.7051810026168823\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=2.4354, train_epoch_loss=0.8901, epcoh time 514.063872616s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/52 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/52 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/52 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/52 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m         #033[0m| 1/52 [00:09<08:19,  9.80s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m         #033[0m| 1/52 [00:09<08:19,  9.80s/it]#015Training Epoch1:   2%|#033[34m         #033[0m| 1/52 [00:09<08:19,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 0.7161707878112793\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   2%|#033[34m         #033[0m| 1/52 [00:09<08:19,  9.80s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.7030244469642639\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m         #033[0m| 2/52 [00:19<08:11,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m         #033[0m| 2/52 [00:19<08:11,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m         #033[0m| 2/52 [00:19<08:11,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   4%|#033[34m         #033[0m| 2/52 [00:19<08:11,  9.83s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m         #033[0m| 3/52 [00:29<08:01,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m         #033[0m| 3/52 [00:29<08:01,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.7345901131629944\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m         #033[0m| 3/52 [00:29<08:02,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   6%|#033[34m         #033[0m| 3/52 [00:29<08:01,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m         #033[0m| 4/52 [00:39<07:52,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.5864450931549072\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m         #033[0m| 4/52 [00:39<07:52,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   8%|#033[34m         #033[0m| 4/52 [00:39<07:52,  9.84s/it]#015Training Epoch1:   8%|#033[34m         #033[0m| 4/52 [00:39<07:52,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m         #033[0m| 5/52 [00:49<07:42,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m         #033[0m| 5/52 [00:49<07:42,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m         #033[0m| 5/52 [00:49<07:42,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 0.7645880579948425\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  10%|#033[34m         #033[0m| 5/52 [00:49<07:42,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m        #033[0m| 6/52 [00:59<07:32,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m        #033[0m| 6/52 [00:59<07:32,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m        #033[0m| 6/52 [00:59<07:32,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 0.6549185514450073\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  12%|#033[34m        #033[0m| 6/52 [00:59<07:32,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m        #033[0m| 7/52 [01:08<07:22,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m        #033[0m| 7/52 [01:08<07:23,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 0.768770694732666\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m        #033[0m| 7/52 [01:08<07:23,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  13%|#033[34m        #033[0m| 7/52 [01:08<07:23,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m        #033[0m| 8/52 [01:18<07:13,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m        #033[0m| 8/52 [01:18<07:13,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 0.6869238018989563\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m        #033[0m| 8/52 [01:18<07:13,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  15%|#033[34m        #033[0m| 8/52 [01:18<07:13,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m        #033[0m| 9/52 [01:28<07:03,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 0.7471967935562134\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m        #033[0m| 9/52 [01:28<07:03,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m        #033[0m| 9/52 [01:28<07:03,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  17%|#033[34m        #033[0m| 9/52 [01:28<07:03,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m        #033[0m| 10/52 [01:38<06:53,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m        #033[0m| 10/52 [01:38<06:53,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m        #033[0m| 10/52 [01:38<06:53,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 0.6849094033241272\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  19%|#033[34m        #033[0m| 10/52 [01:38<06:53,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m        #033[0m| 11/52 [01:48<06:43,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m        #033[0m| 11/52 [01:48<06:43,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m        #033[0m| 11/52 [01:48<06:43,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 0.6975376009941101\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  21%|#033[34m        #033[0m| 11/52 [01:48<06:43,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m       #033[0m| 12/52 [01:58<06:34,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 0.6813499927520752\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m       #033[0m| 12/52 [01:58<06:34,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  23%|#033[34m       #033[0m| 12/52 [01:58<06:34,  9.86s/it]#015Training Epoch1:  23%|#033[34m       #033[0m| 12/52 [01:58<06:34,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m       #033[0m| 13/52 [02:08<06:27,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m       #033[0m| 13/52 [02:08<06:27,  9.93s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 0.7100639343261719\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m       #033[0m| 13/52 [02:08<06:27,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  25%|#033[34m       #033[0m| 13/52 [02:08<06:27,  9.93s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m       #033[0m| 14/52 [02:18<06:16,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m       #033[0m| 14/52 [02:18<06:16,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m       #033[0m| 14/52 [02:18<06:16,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.6279006004333496\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  27%|#033[34m       #033[0m| 14/52 [02:18<06:16,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m       #033[0m| 15/52 [02:27<06:05,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m       #033[0m| 15/52 [02:27<06:05,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 0.6633526086807251\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  29%|#033[34m       #033[0m| 15/52 [02:27<06:05,  9.89s/it]#015Training Epoch1:  29%|#033[34m       #033[0m| 15/52 [02:27<06:05,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 0.6217929720878601\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m       #033[0m| 16/52 [02:37<05:55,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m       #033[0m| 16/52 [02:37<05:55,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m       #033[0m| 16/52 [02:37<05:55,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  31%|#033[34m       #033[0m| 16/52 [02:37<05:55,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 0.7726039886474609\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m      #033[0m| 17/52 [02:47<05:45,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m      #033[0m| 17/52 [02:47<05:45,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  33%|#033[34m      #033[0m| 17/52 [02:47<05:45,  9.87s/it]#015Training Epoch1:  33%|#033[34m      #033[0m| 17/52 [02:47<05:45,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 0.7578521966934204\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m      #033[0m| 18/52 [02:57<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m      #033[0m| 18/52 [02:57<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m      #033[0m| 18/52 [02:57<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  35%|#033[34m      #033[0m| 18/52 [02:57<05:35,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m      #033[0m| 19/52 [03:07<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m      #033[0m| 19/52 [03:07<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m      #033[0m| 19/52 [03:07<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 0.7518867254257202\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  37%|#033[34m      #033[0m| 19/52 [03:07<05:25,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m      #033[0m| 20/52 [03:17<05:15,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.6782746911048889\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m      #033[0m| 20/52 [03:17<05:15,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m      #033[0m| 20/52 [03:17<05:15,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  38%|#033[34m      #033[0m| 20/52 [03:17<05:15,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m      #033[0m| 21/52 [03:27<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m      #033[0m| 21/52 [03:27<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m      #033[0m| 21/52 [03:27<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.6974492073059082\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  40%|#033[34m      #033[0m| 21/52 [03:27<05:05,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m     #033[0m| 22/52 [03:36<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m     #033[0m| 22/52 [03:36<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m     #033[0m| 22/52 [03:36<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 0.646538257598877\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  42%|#033[34m     #033[0m| 22/52 [03:36<04:55,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m     #033[0m| 23/52 [03:46<04:45,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m     #033[0m| 23/52 [03:46<04:45,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.7387104630470276\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m     #033[0m| 23/52 [03:46<04:45,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  44%|#033[34m     #033[0m| 23/52 [03:46<04:45,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m     #033[0m| 24/52 [03:56<04:35,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m     #033[0m| 24/52 [03:56<04:35,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m     #033[0m| 24/52 [03:56<04:35,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 0.6361649036407471\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  46%|#033[34m     #033[0m| 24/52 [03:56<04:35,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m     #033[0m| 25/52 [04:06<04:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m     #033[0m| 25/52 [04:06<04:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m     #033[0m| 25/52 [04:06<04:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.6106300950050354\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  48%|#033[34m     #033[0m| 25/52 [04:06<04:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m     #033[0m| 26/52 [04:16<04:17,  9.90s/it]#015Training Epoch1:  50%|#033[34m     #033[0m| 26/52 [04:16<04:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m     #033[0m| 26/52 [04:16<04:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 0.7093967199325562\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  50%|#033[34m     #033[0m| 26/52 [04:16<04:17,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m    #033[0m| 27/52 [04:26<04:07,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m    #033[0m| 27/52 [04:26<04:07,  9.88s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 0.7214432954788208\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m    #033[0m| 27/52 [04:26<04:07,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  52%|#033[34m    #033[0m| 27/52 [04:26<04:07,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m    #033[0m| 28/52 [04:36<03:56,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m    #033[0m| 28/52 [04:36<03:56,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 0.6846069097518921\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m    #033[0m| 28/52 [04:36<03:56,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  54%|#033[34m    #033[0m| 28/52 [04:36<03:56,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m    #033[0m| 29/52 [04:45<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m    #033[0m| 29/52 [04:45<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m    #033[0m| 29/52 [04:45<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 0.7266582250595093\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  56%|#033[34m    #033[0m| 29/52 [04:45<03:46,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 0.7027044296264648\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m    #033[0m| 30/52 [04:55<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m    #033[0m| 30/52 [04:55<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m    #033[0m| 30/52 [04:55<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  58%|#033[34m    #033[0m| 30/52 [04:55<03:36,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m    #033[0m| 31/52 [05:05<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.7762035131454468\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m    #033[0m| 31/52 [05:05<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m    #033[0m| 31/52 [05:05<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  60%|#033[34m    #033[0m| 31/52 [05:05<03:26,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m   #033[0m| 32/52 [05:15<03:17,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m   #033[0m| 32/52 [05:15<03:17,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 0.6510646939277649\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m   #033[0m| 32/52 [05:15<03:17,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  62%|#033[34m   #033[0m| 32/52 [05:15<03:17,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m   #033[0m| 33/52 [05:25<03:07,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m   #033[0m| 33/52 [05:25<03:07,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m   #033[0m| 33/52 [05:25<03:07,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.6686601042747498\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  63%|#033[34m   #033[0m| 33/52 [05:25<03:07,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 0.6721770167350769\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m   #033[0m| 34/52 [05:35<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m   #033[0m| 34/52 [05:35<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m   #033[0m| 34/52 [05:35<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  65%|#033[34m   #033[0m| 34/52 [05:35<02:57,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m   #033[0m| 35/52 [05:45<02:47,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.5989458560943604\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m   #033[0m| 35/52 [05:45<02:47,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m   #033[0m| 35/52 [05:45<02:47,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  67%|#033[34m   #033[0m| 35/52 [05:45<02:47,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 0.7079333662986755\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m   #033[0m| 36/52 [05:54<02:37,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m   #033[0m| 36/52 [05:54<02:37,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m   #033[0m| 36/52 [05:54<02:37,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  69%|#033[34m   #033[0m| 36/52 [05:54<02:37,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m   #033[0m| 37/52 [06:04<02:27,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 0.6644526124000549\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m   #033[0m| 37/52 [06:04<02:27,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m   #033[0m| 37/52 [06:04<02:27,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  71%|#033[34m   #033[0m| 37/52 [06:04<02:27,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.6390676498413086\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m  #033[0m| 38/52 [06:14<02:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m  #033[0m| 38/52 [06:14<02:17,  9.86s/it]#015Training Epoch1:  73%|#033[34m  #033[0m| 38/52 [06:14<02:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  73%|#033[34m  #033[0m| 38/52 [06:14<02:17,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m  #033[0m| 39/52 [06:24<02:08,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m  #033[0m| 39/52 [06:24<02:08,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m  #033[0m| 39/52 [06:24<02:08,  9.92s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 0.6014181971549988\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  75%|#033[34m  #033[0m| 39/52 [06:24<02:09,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m  #033[0m| 40/52 [06:34<01:58,  9.90s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 0.6154326796531677\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m  #033[0m| 40/52 [06:34<01:58,  9.90s/it]#015Training Epoch1:  77%|#033[34m  #033[0m| 40/52 [06:34<01:58,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  77%|#033[34m  #033[0m| 40/52 [06:34<01:58,  9.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m  #033[0m| 41/52 [06:44<01:48,  9.88s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m  #033[0m| 41/52 [06:44<01:48,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m  #033[0m| 41/52 [06:44<01:48,  9.89s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 0.7035698294639587\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  79%|#033[34m  #033[0m| 41/52 [06:44<01:48,  9.89s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m  #033[0m| 42/52 [06:54<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 0.6961643695831299\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m  #033[0m| 42/52 [06:54<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m  #033[0m| 42/52 [06:54<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  81%|#033[34m  #033[0m| 42/52 [06:54<01:38,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m #033[0m| 43/52 [07:04<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m #033[0m| 43/52 [07:04<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.6768749356269836\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m #033[0m| 43/52 [07:04<01:28,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  83%|#033[34m #033[0m| 43/52 [07:04<01:28,  9.87s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m #033[0m| 44/52 [07:13<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m #033[0m| 44/52 [07:13<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 0.6913503408432007\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  85%|#033[34m #033[0m| 44/52 [07:13<01:18,  9.86s/it]#015Training Epoch1:  85%|#033[34m #033[0m| 44/52 [07:13<01:18,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m #033[0m| 45/52 [07:23<01:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 0.6843655705451965\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m #033[0m| 45/52 [07:23<01:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m #033[0m| 45/52 [07:23<01:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  87%|#033[34m #033[0m| 45/52 [07:23<01:08,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 0.6519650816917419\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m #033[0m| 46/52 [07:33<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m #033[0m| 46/52 [07:33<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m #033[0m| 46/52 [07:33<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  88%|#033[34m #033[0m| 46/52 [07:33<00:59,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m #033[0m| 47/52 [07:43<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m #033[0m| 47/52 [07:43<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m #033[0m| 47/52 [07:43<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 0.7710997462272644\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  90%|#033[34m #033[0m| 47/52 [07:43<00:49,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m#033[0m| 48/52 [07:53<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m#033[0m| 48/52 [07:53<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 0.7189940214157104\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m#033[0m| 48/52 [07:53<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  92%|#033[34m#033[0m| 48/52 [07:53<00:39,  9.84s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m#033[0m| 49/52 [08:03<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 0.7227990627288818\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m#033[0m| 49/52 [08:03<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m#033[0m| 49/52 [08:03<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  94%|#033[34m#033[0m| 49/52 [08:03<00:29,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m#033[0m| 50/52 [08:13<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m#033[0m| 50/52 [08:13<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m#033[0m| 50/52 [08:13<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 0.6939615607261658\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  96%|#033[34m#033[0m| 50/52 [08:13<00:19,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m#033[0m| 51/52 [08:22<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m#033[0m| 51/52 [08:22<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 0.7033200263977051\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m#033[0m| 51/52 [08:22<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:  98%|#033[34m#033[0m| 51/52 [08:22<00:09,  9.85s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m#033[0m| 52/52 [08:32<00:00,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m#033[0m| 52/52 [08:32<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m#033[0m| 52/52 [08:32<00:00,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m#033[0m| 52/52 [08:32<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 0.6999382972717285\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m#033[0m| 52/52 [08:32<00:00,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m#033[0m| 52/52 [08:32<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m#033[0m| 52/52 [08:32<00:00,  9.92s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m#033[0m| 52/52 [08:32<00:00,  9.86s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 2 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/53 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/53 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/53 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/53 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m         #033[0m| 1/53 [00:03<03:05,  3.58s/it]#015evaluating Epoch:   2%|#033[32m         #033[0m| 1/53 [00:03<03:05,  3.57s/it]#015evaluating Epoch:   2%|#033[32m         #033[0m| 1/53 [00:03<03:05,  3.57s/it]#015evaluating Epoch:   2%|#033[32m         #033[0m| 1/53 [00:03<03:05,  3.57s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m         #033[0m| 2/53 [00:07<02:58,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m         #033[0m| 2/53 [00:07<02:58,  3.50s/it]#015evaluating Epoch:   4%|#033[32m         #033[0m| 2/53 [00:07<02:58,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m         #033[0m| 2/53 [00:07<02:58,  3.50s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m         #033[0m| 3/53 [00:10<02:54,  3.48s/it]#015evaluating Epoch:   6%|#033[32m         #033[0m| 3/53 [00:10<02:53,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m         #033[0m| 3/53 [00:10<02:53,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   6%|#033[32m         #033[0m| 3/53 [00:10<02:53,  3.48s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m         #033[0m| 4/53 [00:13<02:50,  3.47s/it]#015evaluating Epoch:   8%|#033[32m         #033[0m| 4/53 [00:13<02:50,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m         #033[0m| 4/53 [00:13<02:50,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m         #033[0m| 4/53 [00:13<02:50,  3.47s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m         #033[0m| 5/53 [00:17<02:46,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m         #033[0m| 5/53 [00:17<02:46,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m         #033[0m| 5/53 [00:17<02:46,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m         #033[0m| 5/53 [00:17<02:46,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m        #033[0m| 6/53 [00:20<02:42,  3.46s/it]#015evaluating Epoch:  11%|#033[32m        #033[0m| 6/53 [00:20<02:42,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m        #033[0m| 6/53 [00:20<02:42,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m        #033[0m| 6/53 [00:20<02:42,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m        #033[0m| 7/53 [00:24<02:38,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m        #033[0m| 7/53 [00:24<02:38,  3.46s/it]#015evaluating Epoch:  13%|#033[32m        #033[0m| 7/53 [00:24<02:38,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m        #033[0m| 7/53 [00:24<02:38,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m        #033[0m| 8/53 [00:27<02:35,  3.45s/it]#015evaluating Epoch:  15%|#033[32m        #033[0m| 8/53 [00:27<02:35,  3.45s/it]#015evaluating Epoch:  15%|#033[32m        #033[0m| 8/53 [00:27<02:35,  3.45s/it]#015evaluating Epoch:  15%|#033[32m        #033[0m| 8/53 [00:27<02:35,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m        #033[0m| 9/53 [00:31<02:32,  3.45s/it]#015evaluating Epoch:  17%|#033[32m        #033[0m| 9/53 [00:31<02:32,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m        #033[0m| 9/53 [00:31<02:32,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m        #033[0m| 9/53 [00:31<02:32,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m        #033[0m| 10/53 [00:34<02:28,  3.45s/it]#015evaluating Epoch:  19%|#033[32m        #033[0m| 10/53 [00:34<02:28,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m        #033[0m| 10/53 [00:34<02:28,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  19%|#033[32m        #033[0m| 10/53 [00:34<02:28,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m        #033[0m| 11/53 [00:38<02:24,  3.45s/it]#015evaluating Epoch:  21%|#033[32m        #033[0m| 11/53 [00:38<02:24,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m        #033[0m| 11/53 [00:38<02:24,  3.45s/it]#015evaluating Epoch:  21%|#033[32m        #033[0m| 11/53 [00:38<02:24,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m       #033[0m| 12/53 [00:41<02:21,  3.45s/it]#015evaluating Epoch:  23%|#033[32m       #033[0m| 12/53 [00:41<02:21,  3.45s/it]#015evaluating Epoch:  23%|#033[32m       #033[0m| 12/53 [00:41<02:21,  3.45s/it]#015evaluating Epoch:  23%|#033[32m       #033[0m| 12/53 [00:41<02:21,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m       #033[0m| 13/53 [00:44<02:18,  3.45s/it]#015evaluating Epoch:  25%|#033[32m       #033[0m| 13/53 [00:44<02:18,  3.45s/it]#015evaluating Epoch:  25%|#033[32m       #033[0m| 13/53 [00:44<02:18,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m       #033[0m| 13/53 [00:45<02:18,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m       #033[0m| 14/53 [00:48<02:14,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m       #033[0m| 14/53 [00:48<02:14,  3.45s/it]#015evaluating Epoch:  26%|#033[32m       #033[0m| 14/53 [00:48<02:14,  3.45s/it]#015evaluating Epoch:  26%|#033[32m       #033[0m| 14/53 [00:48<02:14,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m       #033[0m| 15/53 [00:51<02:11,  3.45s/it]#015evaluating Epoch:  28%|#033[32m       #033[0m| 15/53 [00:51<02:11,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m       #033[0m| 15/53 [00:51<02:11,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m       #033[0m| 15/53 [00:51<02:11,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m       #033[0m| 16/53 [00:55<02:07,  3.45s/it]#015evaluating Epoch:  30%|#033[32m       #033[0m| 16/53 [00:55<02:07,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m       #033[0m| 16/53 [00:55<02:07,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m       #033[0m| 16/53 [00:55<02:07,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m      #033[0m| 17/53 [00:58<02:04,  3.45s/it]#015evaluating Epoch:  32%|#033[32m      #033[0m| 17/53 [00:58<02:04,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m      #033[0m| 17/53 [00:58<02:04,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m      #033[0m| 17/53 [00:58<02:04,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m      #033[0m| 18/53 [01:02<02:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m      #033[0m| 18/53 [01:02<02:00,  3.45s/it]#015evaluating Epoch:  34%|#033[32m      #033[0m| 18/53 [01:02<02:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m      #033[0m| 18/53 [01:02<02:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m      #033[0m| 19/53 [01:05<01:57,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m      #033[0m| 19/53 [01:05<01:57,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m      #033[0m| 19/53 [01:05<01:57,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m      #033[0m| 19/53 [01:05<01:57,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m      #033[0m| 20/53 [01:09<01:53,  3.45s/it]#015evaluating Epoch:  38%|#033[32m      #033[0m| 20/53 [01:09<01:53,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m      #033[0m| 20/53 [01:09<01:53,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m      #033[0m| 20/53 [01:09<01:53,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m      #033[0m| 21/53 [01:12<01:50,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m      #033[0m| 21/53 [01:12<01:50,  3.45s/it]#015evaluating Epoch:  40%|#033[32m      #033[0m| 21/53 [01:12<01:50,  3.45s/it]#015evaluating Epoch:  40%|#033[32m      #033[0m| 21/53 [01:12<01:50,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m     #033[0m| 22/53 [01:16<01:47,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m     #033[0m| 22/53 [01:16<01:47,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m     #033[0m| 22/53 [01:16<01:47,  3.45s/it]#015evaluating Epoch:  42%|#033[32m     #033[0m| 22/53 [01:16<01:47,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m     #033[0m| 23/53 [01:19<01:43,  3.45s/it]#015evaluating Epoch:  43%|#033[32m     #033[0m| 23/53 [01:19<01:43,  3.45s/it]#015evaluating Epoch:  43%|#033[32m     #033[0m| 23/53 [01:19<01:43,  3.45s/it]#015evaluating Epoch:  43%|#033[32m     #033[0m| 23/53 [01:19<01:43,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m     #033[0m| 24/53 [01:22<01:40,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m     #033[0m| 24/53 [01:22<01:40,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m     #033[0m| 24/53 [01:22<01:40,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m     #033[0m| 24/53 [01:22<01:40,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m     #033[0m| 25/53 [01:26<01:36,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m     #033[0m| 25/53 [01:26<01:36,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m     #033[0m| 25/53 [01:26<01:36,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m     #033[0m| 25/53 [01:26<01:36,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m     #033[0m| 26/53 [01:29<01:33,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m     #033[0m| 26/53 [01:29<01:33,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m     #033[0m| 26/53 [01:29<01:33,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m     #033[0m| 26/53 [01:29<01:33,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m     #033[0m| 27/53 [01:33<01:29,  3.45s/it]#015evaluating Epoch:  51%|#033[32m     #033[0m| 27/53 [01:33<01:29,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m     #033[0m| 27/53 [01:33<01:29,  3.45s/it]#015evaluating Epoch:  51%|#033[32m     #033[0m| 27/53 [01:33<01:29,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m    #033[0m| 28/53 [01:36<01:26,  3.45s/it]#015evaluating Epoch:  53%|#033[32m    #033[0m| 28/53 [01:36<01:26,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m    #033[0m| 28/53 [01:36<01:26,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m    #033[0m| 28/53 [01:36<01:26,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m    #033[0m| 29/53 [01:40<01:22,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m    #033[0m| 29/53 [01:40<01:22,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m    #033[0m| 29/53 [01:40<01:22,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m    #033[0m| 29/53 [01:40<01:22,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m    #033[0m| 30/53 [01:43<01:19,  3.45s/it]#015evaluating Epoch:  57%|#033[32m    #033[0m| 30/53 [01:43<01:19,  3.45s/it]#015evaluating Epoch:  57%|#033[32m    #033[0m| 30/53 [01:43<01:19,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m    #033[0m| 30/53 [01:43<01:19,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m    #033[0m| 31/53 [01:47<01:15,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m    #033[0m| 31/53 [01:47<01:15,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m    #033[0m| 31/53 [01:47<01:15,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m    #033[0m| 31/53 [01:47<01:15,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m    #033[0m| 32/53 [01:50<01:12,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m    #033[0m| 32/53 [01:50<01:12,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m    #033[0m| 32/53 [01:50<01:12,  3.45s/it]#015evaluating Epoch:  60%|#033[32m    #033[0m| 32/53 [01:50<01:12,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m   #033[0m| 33/53 [01:54<01:09,  3.45s/it]#015evaluating Epoch:  62%|#033[32m   #033[0m| 33/53 [01:54<01:09,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m   #033[0m| 33/53 [01:54<01:09,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m   #033[0m| 33/53 [01:54<01:09,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m   #033[0m| 34/53 [01:57<01:05,  3.45s/it]#015evaluating Epoch:  64%|#033[32m   #033[0m| 34/53 [01:57<01:05,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m   #033[0m| 34/53 [01:57<01:05,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m   #033[0m| 34/53 [01:57<01:05,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m   #033[0m| 35/53 [02:00<01:02,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m   #033[0m| 35/53 [02:00<01:02,  3.45s/it]#015evaluating Epoch:  66%|#033[32m   #033[0m| 35/53 [02:00<01:02,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m   #033[0m| 35/53 [02:00<01:02,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m   #033[0m| 36/53 [02:04<00:58,  3.45s/it]#015evaluating Epoch:  68%|#033[32m   #033[0m| 36/53 [02:04<00:58,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m   #033[0m| 36/53 [02:04<00:58,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m   #033[0m| 36/53 [02:04<00:58,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m   #033[0m| 37/53 [02:07<00:55,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m   #033[0m| 37/53 [02:07<00:55,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m   #033[0m| 37/53 [02:07<00:55,  3.45s/it]#015evaluating Epoch:  70%|#033[32m   #033[0m| 37/53 [02:07<00:55,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m  #033[0m| 38/53 [02:11<00:51,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m  #033[0m| 38/53 [02:11<00:51,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m  #033[0m| 38/53 [02:11<00:51,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m  #033[0m| 38/53 [02:11<00:51,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m  #033[0m| 39/53 [02:14<00:48,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m  #033[0m| 39/53 [02:14<00:48,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m  #033[0m| 39/53 [02:14<00:48,  3.45s/it]#015evaluating Epoch:  74%|#033[32m  #033[0m| 39/53 [02:14<00:48,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m  #033[0m| 40/53 [02:18<00:44,  3.45s/it]#015evaluating Epoch:  75%|#033[32m  #033[0m| 40/53 [02:18<00:44,  3.45s/it]#015evaluating Epoch:  75%|#033[32m  #033[0m| 40/53 [02:18<00:44,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m  #033[0m| 40/53 [02:18<00:44,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m  #033[0m| 41/53 [02:21<00:41,  3.45s/it]#015evaluating Epoch:  77%|#033[32m  #033[0m| 41/53 [02:21<00:41,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m  #033[0m| 41/53 [02:21<00:41,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m  #033[0m| 41/53 [02:21<00:41,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m  #033[0m| 42/53 [02:25<00:37,  3.45s/it]#015evaluating Epoch:  79%|#033[32m  #033[0m| 42/53 [02:25<00:37,  3.45s/it]#015evaluating Epoch:  79%|#033[32m  #033[0m| 42/53 [02:25<00:37,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m  #033[0m| 42/53 [02:25<00:37,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m  #033[0m| 43/53 [02:28<00:34,  3.45s/it]#015evaluating Epoch:  81%|#033[32m  #033[0m| 43/53 [02:28<00:34,  3.45s/it]#015evaluating Epoch:  81%|#033[32m  #033[0m| 43/53 [02:28<00:34,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  81%|#033[32m  #033[0m| 43/53 [02:28<00:34,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m #033[0m| 44/53 [02:31<00:31,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m #033[0m| 44/53 [02:31<00:31,  3.45s/it]#015evaluating Epoch:  83%|#033[32m #033[0m| 44/53 [02:31<00:31,  3.45s/it]#015evaluating Epoch:  83%|#033[32m #033[0m| 44/53 [02:31<00:31,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m #033[0m| 45/53 [02:35<00:27,  3.45s/it]#015evaluating Epoch:  85%|#033[32m #033[0m| 45/53 [02:35<00:27,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m #033[0m| 45/53 [02:35<00:27,  3.45s/it]#015evaluating Epoch:  85%|#033[32m #033[0m| 45/53 [02:35<00:27,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m #033[0m| 46/53 [02:38<00:24,  3.45s/it]#015evaluating Epoch:  87%|#033[32m #033[0m| 46/53 [02:38<00:24,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m #033[0m| 46/53 [02:38<00:24,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m #033[0m| 46/53 [02:38<00:24,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m #033[0m| 47/53 [02:42<00:20,  3.45s/it]#015evaluating Epoch:  89%|#033[32m #033[0m| 47/53 [02:42<00:20,  3.45s/it]#015evaluating Epoch:  89%|#033[32m #033[0m| 47/53 [02:42<00:20,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m #033[0m| 47/53 [02:42<00:20,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m #033[0m| 48/53 [02:45<00:17,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m #033[0m| 48/53 [02:45<00:17,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m #033[0m| 48/53 [02:45<00:17,  3.45s/it]#015evaluating Epoch:  91%|#033[32m #033[0m| 48/53 [02:45<00:17,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m#033[0m| 49/53 [02:49<00:13,  3.46s/it]#015evaluating Epoch:  92%|#033[32m#033[0m| 49/53 [02:49<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m#033[0m| 49/53 [02:49<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m#033[0m| 49/53 [02:49<00:13,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m#033[0m| 50/53 [02:52<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m#033[0m| 50/53 [02:52<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m#033[0m| 50/53 [02:52<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  94%|#033[32m#033[0m| 50/53 [02:52<00:10,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m#033[0m| 51/53 [02:56<00:06,  3.46s/it]#015evaluating Epoch:  96%|#033[32m#033[0m| 51/53 [02:56<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m#033[0m| 51/53 [02:56<00:06,  3.46s/it]#015evaluating Epoch:  96%|#033[32m#033[0m| 51/53 [02:56<00:06,  3.46s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m#033[0m| 52/53 [02:59<00:03,  3.45s/it]#015evaluating Epoch:  98%|#033[32m#033[0m| 52/53 [02:59<00:03,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m#033[0m| 52/53 [02:59<00:03,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m#033[0m| 52/53 [02:59<00:03,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.45s/it]#015evaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m#033[0m| 53/53 [03:03<00:00,  3.45s/it]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(1.9598, device='cuda:0') eval_epoch_loss=tensor(0.6728, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 0.6728191375732422\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=1.9959, train_epoch_loss=0.6911, epcoh time 513.4728931119998s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 2.215643882751465\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 0.7905999422073364\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 1.9919836521148682\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 0.6890000700950623\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 513.7683828639999\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 3.5007775834999393\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|     | 1/2 [00:00<00:00,  1.19it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:01<00:00,  1.80it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|| 2/2 [00:01<00:00,  1.67it/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2023-12-05 07:18:05,836 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-12-05 07:18:05,836 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-12-05 07:18:05,837 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-12-05 07:18:08 Uploading - Uploading generated training model\n",
      "2023-12-05 07:18:46 Completed - Training job completed\n",
      "Training seconds: 2109\n",
      "Billable seconds: 2109\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": train_data_location},wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47e65c3e-0662-4ce6-bc20-b7819f751d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.2xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-2-7b-2023-12-05-07-40-44-908\n",
      "INFO:sagemaker:Creating endpoint-config with name meta-textgeneration-llama-2-7b-2023-12-05-07-40-44-906\n",
      "INFO:sagemaker:Creating endpoint with name meta-textgeneration-llama-2-7b-2023-12-05-07-40-44-906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc732fe1-dab7-4ed4-b122-2876707ddf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
